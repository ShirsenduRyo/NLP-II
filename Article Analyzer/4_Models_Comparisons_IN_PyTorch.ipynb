{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "article = '''Mumbai: Analysts tracking Bajaj Finance reiterated their bullish outlook after the Pune-based non-bank lender 's first-quarter business growth update took the Street by surprise, prompting brokerage houses to upgrade their recommendations and raise their respective price targets. Shares of Bajaj Finance rose as high as 8% to post their biggest single-day gain in nearly one year. The stock surged to near two-year high on Tuesday to close at ₹7,868 apiece on the NSE, up 7.3% from the previous close, extending its run of gains to six consecutive sessions. CLSA raised EPS estimates by 5-6% and upgraded the stock to buy. The global brokerage expects the stock to rise another 15% from the current levels.\"Bajaj Finance reported very strong pre-quarter numbers for Q1FY24,\" said CLSA in a client note. \"While expected 6-7% QoQ AUM growth, the company delivered 9%. New customer acquisition healthy and volume growth in disbursements a strong 34% YoY.\"Morgan Stanley raised its price target which has the potential to deliver another 18% returns from the current levels.\"AUM growth of 9.2% QoQ, 32% YoY with value & vol growth & strong customer acquisition should dispel loan growth debate,\" Morgan Stanley told clients in a note. \"Bajaj Finance has a strong credit track record; RoA is at a historical high. See this large liquid stock re-rating to 30x F25e P/E.\"Bajaj Finance's new loans booked during Q1 grew by 34% to 9.94 million as compared to 7.42 million in the corresponding quarter of the previous year. Assets under management (AUM) grew 32% YoY - the highest-ever quarterly increase - to ₹2.7 lakh crore.The customer franchise stood at 72.98 million at the end of June 2023 as compared to 60.30 million as of June 2022 after seeing the highest-ever quarterly increase.BofA Securities also raised its price target. It said the first quarter update should also reinforce confidence in its long-term growth guidance of 25-27%. \"The company is on track to beat the upper end of its 11-12 million customer acquisition guidance comfortably,\" the firm said.'''"
      ],
      "metadata": {
        "id": "UhUxASiLK946"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install transformers"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hnLYV2MhYlWG",
        "outputId": "68da63fd-2f49-4f67-fe46-754535fd9bb4"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: transformers in /usr/local/lib/python3.10/dist-packages (4.30.2)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers) (3.12.2)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.14.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.16.4)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (1.22.4)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (23.1)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (6.0)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (2022.10.31)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers) (2.27.1)\n",
            "Requirement already satisfied: tokenizers!=0.11.3,<0.14,>=0.11.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.13.3)\n",
            "Requirement already satisfied: safetensors>=0.3.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.3.1)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers) (4.65.0)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.14.1->transformers) (2023.6.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.14.1->transformers) (4.7.1)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (1.26.16)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2023.5.7)\n",
            "Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2.0.12)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.4)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 1. GPT2"
      ],
      "metadata": {
        "id": "FC0HvREAWkp4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from transformers import GPT2Tokenizer, GPT2ForSequenceClassification, GPT2LMHeadModel"
      ],
      "metadata": {
        "id": "Gj-Qwf0oLLke"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "F6OwiIWeKwYU"
      },
      "outputs": [],
      "source": [
        "\n",
        "\n",
        "class GPT2ArticleClassifier:\n",
        "    def __init__(self):\n",
        "        self.model_name = 'gpt2'\n",
        "        self.model = GPT2ForSequenceClassification.from_pretrained(self.model_name)\n",
        "        self.tokenizer = GPT2Tokenizer.from_pretrained(self.model_name)\n",
        "        self.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "        self.labels = ['adverse', 'growth']\n",
        "        self.model.to(self.device)\n",
        "\n",
        "    def summarize_article(self, article, max_length=100):\n",
        "        model_name = 'gpt2'\n",
        "        model = GPT2LMHeadModel.from_pretrained(model_name)\n",
        "        tokenizer = GPT2Tokenizer.from_pretrained(model_name)\n",
        "        device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "        model.to(device)\n",
        "\n",
        "        inputs = tokenizer.encode(article, return_tensors='pt')\n",
        "        inputs = inputs.to(device)\n",
        "\n",
        "        with torch.no_grad():\n",
        "            outputs = model.generate(inputs, max_length=max_length, num_return_sequences=1)\n",
        "            summarized_text = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
        "\n",
        "        return summarized_text\n",
        "\n",
        "    def classify_theme(self, article):\n",
        "        inputs = self.tokenizer(article, return_tensors='pt')\n",
        "        inputs = inputs.to(self.device)\n",
        "\n",
        "        with torch.no_grad():\n",
        "            outputs = self.model(**inputs)\n",
        "            logits = outputs.logits\n",
        "            probabilities = torch.softmax(logits, dim=1)\n",
        "            predicted_label_id = torch.argmax(logits, dim=1).item()\n",
        "            predicted_label = self.labels[predicted_label_id]\n",
        "            growth_probability = probabilities[0][1].item()\n",
        "            adverse_probability = probabilities[0][0].item()\n",
        "\n",
        "        print(\"Growth Probability:\", growth_probability)\n",
        "        print(\"adverse Probability:\", adverse_probability)\n",
        "\n",
        "        return predicted_label\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Example usage\n",
        "article = article\n",
        "\n",
        "classifier = GPT2ArticleClassifier()\n",
        "predicted_label = classifier.classify_theme(article)\n",
        "print(\"Predicted Label:\", predicted_label)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-eZ34J3qNWc-",
        "outputId": "d051211d-372d-4f6b-9a19-513cb819da6f"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of GPT2ForSequenceClassification were not initialized from the model checkpoint at gpt2 and are newly initialized: ['score.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Growth Probability: 0.10013208538293839\n",
            "adverse Probability: 0.8998678922653198\n",
            "Predicted Label: adverse\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "summarized_text = classifier.summarize_article(article)\n",
        "print(\"Summarized Text:\", summarized_text)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "h2v89NZmK8d3",
        "outputId": "229c7cd9-30f8-4499-c2b8-6f68fe8c89c9"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Input length of input_ids is 477, but `max_length` is set to 100. This can lead to unexpected behavior. You should consider increasing `max_new_tokens`.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Summarized Text: Mumbai: Analysts tracking Bajaj Finance reiterated their bullish outlook after the Pune-based non-bank lender's first-quarter business growth update took the Street by surprise, prompting brokerage houses to upgrade their recommendations and raise their respective price targets. Shares of Bajaj Finance rose as high as 8% to post their biggest single-day gain in nearly one year. The stock surged to near two-year high on Tuesday to close at ₹7,868 apiece on the NSE, up 7.3% from the previous close, extending its run of gains to six consecutive sessions. CLSA raised EPS estimates by 5-6% and upgraded the stock to buy. The global brokerage expects the stock to rise another 15% from the current levels.\"Bajaj Finance reported very strong pre-quarter numbers for Q1FY24,\" said CLSA in a client note. \"While expected 6-7% QoQ AUM growth, the company delivered 9%. New customer acquisition healthy and volume growth in disbursements a strong 34% YoY.\"Morgan Stanley raised its price target which has the potential to deliver another 18% returns from the current levels.\"AUM growth of 9.2% QoQ, 32% YoY with value & vol growth & strong customer acquisition should dispel loan growth debate,\" Morgan Stanley told clients in a note. \"Bajaj Finance has a strong credit track record; RoA is at a historical high. See this large liquid stock re-rating to 30x F25e P/E.\"Bajaj Finance's new loans booked during Q1 grew by 34% to 9.94 million as compared to 7.42 million in the corresponding quarter of the previous year. Assets under management (AUM) grew 32% YoY - the highest-ever quarterly increase - to ₹2.7 lakh crore.The customer franchise stood at 72.98 million at the end of June 2023 as compared to 60.30 million as of June 2022 after seeing the highest-ever quarterly increase.BofA Securities also raised its price target. It said the first quarter update should also reinforce confidence in its long-term growth guidance of 25-27%. \"The company is on track to beat the upper end of its 11-12 million customer acquisition guidance comfortably,\" the firm said. \"\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 2. Roberta"
      ],
      "metadata": {
        "id": "PwW4ftX8XMwS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import RobertaTokenizer, RobertaForSequenceClassification\n",
        "\n",
        "class RobertaArticleClassifier:\n",
        "    def __init__(self):\n",
        "        self.model_name = 'roberta-base'\n",
        "        self.model = RobertaForSequenceClassification.from_pretrained(self.model_name)\n",
        "        self.tokenizer = RobertaTokenizer.from_pretrained(self.model_name)\n",
        "        self.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "        self.labels = ['adverse', 'growth']\n",
        "        self.model.to(self.device)\n",
        "\n",
        "    def classify_theme(self, article):\n",
        "        inputs = self.tokenizer.encode_plus(article, return_tensors='pt', truncation=True, padding=True)\n",
        "        inputs = inputs.to(self.device)\n",
        "\n",
        "        with torch.no_grad():\n",
        "            outputs = self.model(**inputs)\n",
        "            logits = outputs.logits\n",
        "            probabilities = torch.softmax(logits, dim=1)\n",
        "            predicted_label_id = torch.argmax(logits, dim=1).item()\n",
        "            predicted_label = self.labels[predicted_label_id]\n",
        "            growth_probability = probabilities[0][1].item()\n",
        "            adverse_probability = probabilities[0][0].item()\n",
        "\n",
        "        print(\"Growth Probability:\", growth_probability)\n",
        "        print(\"adverse Probability:\", adverse_probability)\n",
        "\n",
        "        return predicted_label\n",
        "\n",
        "# Example usage\n",
        "article = article\n",
        "\n",
        "classifier = RobertaArticleClassifier()\n",
        "predicted_label = classifier.classify_theme(article)\n",
        "print(\"Predicted Label:\", predicted_label)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8FltmXZyYhH5",
        "outputId": "12c4e91d-379f-4018-8fbf-f3af56296b08"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of the model checkpoint at roberta-base were not used when initializing RobertaForSequenceClassification: ['lm_head.bias', 'lm_head.layer_norm.bias', 'lm_head.dense.bias', 'lm_head.dense.weight', 'lm_head.layer_norm.weight']\n",
            "- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier.out_proj.bias', 'classifier.out_proj.weight', 'classifier.dense.bias', 'classifier.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Growth Probability: 0.5078659653663635\n",
            "adverse Probability: 0.4921340048313141\n",
            "Predicted Label: growth\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 3. Bert UnCased"
      ],
      "metadata": {
        "id": "j3HwYEcIjeIE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from transformers import BertTokenizer, BertForSequenceClassification\n",
        "\n",
        "class BertArticleClassifier:\n",
        "    def __init__(self):\n",
        "        self.model_name = 'bert-large-uncased'\n",
        "        self.model = BertForSequenceClassification.from_pretrained(self.model_name)\n",
        "        self.tokenizer = BertTokenizer.from_pretrained(self.model_name)\n",
        "        self.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "        self.labels = ['adverse', 'growth']\n",
        "        self.actionable_labels = ['not_actionable', 'actionable']\n",
        "        self.model.to(self.device)\n",
        "\n",
        "    def classify_theme(self, article):\n",
        "        inputs = self.tokenizer.encode_plus(article, return_tensors='pt', truncation=True, padding=True)\n",
        "        inputs = inputs.to(self.device)\n",
        "\n",
        "        with torch.no_grad():\n",
        "            outputs = self.model(**inputs)\n",
        "            logits = outputs.logits\n",
        "            probabilities = torch.softmax(logits, dim=1)\n",
        "            predicted_label_id = torch.argmax(logits, dim=1).item()\n",
        "            predicted_label = self.labels[predicted_label_id]\n",
        "            growth_probability = probabilities[0][1].item()\n",
        "            adverse_probability = probabilities[0][0].item()\n",
        "\n",
        "        print(\"Growth Probability:\", growth_probability)\n",
        "        print(\"Adverse Probability:\", adverse_probability)\n",
        "\n",
        "    def classify_actionable(self, text):\n",
        "        inputs = self.tokenizer.encode_plus(text, return_tensors='pt', truncation=True, padding=True)\n",
        "        inputs = inputs.to(self.device)\n",
        "\n",
        "        with torch.no_grad():\n",
        "            outputs = self.model(**inputs)\n",
        "            logits = outputs.logits\n",
        "            probabilities = torch.softmax(logits, dim=1)\n",
        "            predicted_label_id = torch.argmax(logits, dim=1).item()\n",
        "            predicted_label = self.actionable_labels[predicted_label_id]\n",
        "            not_actionable_probability = probabilities[0][0].item()\n",
        "            actionable_probability = probabilities[0][1].item()\n",
        "\n",
        "        print(\"Not Actionable Probability:\", not_actionable_probability)\n",
        "        print(\"Actionable Probability:\", actionable_probability)\n",
        "\n",
        "        return predicted_label"
      ],
      "metadata": {
        "id": "CYDwms8cYiJX"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "classifier = BertArticleClassifier()\n",
        "predicted_label = classifier.classify_theme(article)\n",
        "print(\"Predicted Label:\", predicted_label)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "G1WLd3fwvov5",
        "outputId": "efc40efb-c1d0-4af5-d8e9-8f9059fd8aa7"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of the model checkpoint at bert-large-uncased were not used when initializing BertForSequenceClassification: ['cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.bias', 'cls.seq_relationship.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.bias']\n",
            "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-large-uncased and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Growth Probability: 0.3709351420402527\n",
            "Adverse Probability: 0.6290649175643921\n",
            "Predicted Label: None\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "predicted_label = classifier.classify_actionable(article)\n",
        "print(\"Predicted Label:\", predicted_label)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "D-DbPaPTgUGW",
        "outputId": "4b8da21f-a829-4f91-fb85-f88ead1abdd3"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Not Actionable Probability: 0.6290649175643921\n",
            "Actionable Probability: 0.3709351420402527\n",
            "Predicted Label: not_actionable\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import BartTokenizer, BartForConditionalGeneration"
      ],
      "metadata": {
        "id": "yyGgoxKLeLCf"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class ArticleGistGenerator:\n",
        "    def __init__(self):\n",
        "        self.model_name = 'facebook/bart-large-cnn'\n",
        "        self.tokenizer = BartTokenizer.from_pretrained(self.model_name)\n",
        "        self.model = BartForConditionalGeneration.from_pretrained(self.model_name)\n",
        "\n",
        "    def generate_gist(self, article):\n",
        "        inputs = self.tokenizer([article], max_length=1024, truncation=True, return_tensors='pt')\n",
        "        summary_ids = self.model.generate(inputs['input_ids'], num_beams=4, max_length=100, early_stopping=True)\n",
        "        gist = self.tokenizer.decode(summary_ids[0], skip_special_tokens=True)\n",
        "        return gist"
      ],
      "metadata": {
        "id": "rAl41k79cIzT"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "classifier = ArticleGistGenerator()\n",
        "predicted_theme = classifier.generate_gist(article)\n",
        "print(predicted_theme)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "B02BaV0fcIvq",
        "outputId": "6c31ea67-d30a-4df9-9bb1-ea8574930814"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Shares of Bajaj Finance rose as high as 8% to post their biggest single-day gain in nearly one year. The stock surged to near two-year high on Tuesday to close at ₹7,868 apiece on the NSE. Morgan Stanley raised its price target which has the potential to deliver another 18% returns from current levels.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "OfQCYRTecIsU"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "EbagxVFKaLHi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class BertArticleClassifier2:\n",
        "    def __init__(self):\n",
        "        self.model_name = 'bert-large-cased'\n",
        "        self.model = BertForSequenceClassification.from_pretrained(self.model_name)\n",
        "        self.tokenizer = BertTokenizer.from_pretrained(self.model_name)\n",
        "        self.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "        #self.labels = ['adverse', 'growth']\n",
        "        self.actionable_labels = ['not_actionable', 'actionable']\n",
        "        self.model.to(self.device)\n",
        "\n",
        "    # def classify_theme(self, article):\n",
        "    #     inputs = self.tokenizer.encode_plus(article, return_tensors='pt', truncation=True, padding=True)\n",
        "    #     inputs = inputs.to(self.device)\n",
        "\n",
        "    #     with torch.no_grad():\n",
        "    #         outputs = self.model(**inputs)\n",
        "    #         logits = outputs.logits\n",
        "    #         probabilities = torch.softmax(logits, dim=1)\n",
        "    #         predicted_label_id = torch.argmax(logits, dim=1).item()\n",
        "    #         predicted_label = self.labels[predicted_label_id]\n",
        "    #         growth_probability = probabilities[0][1].item()\n",
        "    #         adverse_probability = probabilities[0][0].item()\n",
        "\n",
        "    #     print(\"Growth Probability:\", growth_probability)\n",
        "    #     print(\"adverse Probability:\", adverse_probability)\n",
        "\n",
        "     #   return predicted_label\n",
        "    def classify_actionable(self, text):\n",
        "        inputs = self.tokenizer.encode_plus(text, return_tensors='pt', truncation=True, padding=True)\n",
        "        inputs = inputs.to(self.device)\n",
        "\n",
        "        with torch.no_grad():\n",
        "            outputs = self.model(**inputs)\n",
        "            logits = outputs.logits\n",
        "            probabilities = torch.softmax(logits, dim=1)\n",
        "            predicted_label_id = torch.argmax(logits, dim=1).item()\n",
        "            predicted_label = self.actionable_labels[predicted_label_id]\n",
        "            not_actionable_probability = probabilities[0][0].item()\n",
        "            actionable_probability = probabilities[0][1].item()\n",
        "\n",
        "        print(\"Not Actionable Probability:\", not_actionable_probability)\n",
        "        print(\"Actionable Probability:\", actionable_probability)\n",
        "\n",
        "        return predicted_label"
      ],
      "metadata": {
        "id": "x42GX3V3ZQFm"
      },
      "execution_count": 40,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "classifier = BertArticleClassifier2()\n",
        "#predicted_label = classifier.classify_theme(article)\n",
        "#print(\"Predicted Label:\", predicted_label)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2AciSckAZPnn",
        "outputId": "582373ae-2924-4f3f-aed6-24e9ad0d3ad1"
      },
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of the model checkpoint at bert-large-cased were not used when initializing BertForSequenceClassification: ['cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.bias', 'cls.seq_relationship.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.bias']\n",
            "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-large-cased and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "predicted_label = classifier.classify_actionable(article)\n",
        "print(\"Predicted Label:\", predicted_label)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ytTF_baLlNLX",
        "outputId": "98942219-0c63-4fd4-9205-0dfa9b643010"
      },
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Not Actionable Probability: 0.6418688893318176\n",
            "Actionable Probability: 0.3581310510635376\n",
            "Predicted Label: not_actionable\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 5. DistillBert"
      ],
      "metadata": {
        "id": "YQHUtDM8Xd3I"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from transformers import DistilBertTokenizer, DistilBertForSequenceClassification\n",
        "\n",
        "class DistilBertArticleClassifier:\n",
        "    def __init__(self):\n",
        "        self.model_name = 'distilbert-base-uncased'\n",
        "        self.model = DistilBertForSequenceClassification.from_pretrained(self.model_name)\n",
        "        self.tokenizer = DistilBertTokenizer.from_pretrained(self.model_name)\n",
        "        self.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "        self.labels = ['adverse', 'growth']\n",
        "        self.model.to(self.device)\n",
        "\n",
        "    def classify_theme(self, article):\n",
        "        inputs = self.tokenizer.encode_plus(article, return_tensors='pt', truncation=True, padding=True)\n",
        "        inputs = inputs.to(self.device)\n",
        "\n",
        "        with torch.no_grad():\n",
        "            outputs = self.model(**inputs)\n",
        "            logits = outputs.logits\n",
        "            probabilities = torch.softmax(logits, dim=1)\n",
        "            predicted_label_id = torch.argmax(logits, dim=1).item()\n",
        "            predicted_label = self.labels[predicted_label_id]\n",
        "            growth_probability = probabilities[0][1].item()\n",
        "            adverse_probability = probabilities[0][0].item()\n",
        "\n",
        "        print(\"Growth Probability:\", growth_probability)\n",
        "        print(\"adverse Probability:\", adverse_probability)\n",
        "\n",
        "        return predicted_label\n",
        "\n",
        "# Example usage\n",
        "# article = \"Your article text goes here\"\n",
        "\n",
        "classifier = DistilBertArticleClassifier()\n",
        "predicted_label = classifier.classify_theme(article)\n",
        "print(\"Predicted Label:\", predicted_label)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "67H6UDDlwEWv",
        "outputId": "65bdccf9-445d-4b9f-f0f8-2605cb596a18"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of the model checkpoint at distilbert-base-uncased were not used when initializing DistilBertForSequenceClassification: ['vocab_layer_norm.bias', 'vocab_projector.bias', 'vocab_transform.weight', 'vocab_layer_norm.weight', 'vocab_transform.bias']\n",
            "- This IS expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['pre_classifier.weight', 'classifier.weight', 'pre_classifier.bias', 'classifier.bias']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Growth Probability: 0.4986215829849243\n",
            "adverse Probability: 0.5013784766197205\n",
            "Predicted Label: adverse\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from transformers import ElectraTokenizer, ElectraForSequenceClassification\n",
        "\n",
        "class ArticleClassifier:\n",
        "    def __init__(self):\n",
        "        self.model_name = 'google/electra-base-discriminator'\n",
        "        self.tokenizer = ElectraTokenizer.from_pretrained(self.model_name)\n",
        "        self.model = ElectraForSequenceClassification.from_pretrained(self.model_name)\n",
        "        self.label_map = {0: 'Not Growth', 1: 'Growth'}\n",
        "\n",
        "    def classify(self, article):\n",
        "        inputs = self.tokenizer.encode_plus(\n",
        "            article,\n",
        "            add_special_tokens=True,\n",
        "            padding='longest',\n",
        "            truncation=True,\n",
        "            return_tensors='pt'\n",
        "        )\n",
        "        outputs = self.model(**inputs)\n",
        "        logits = outputs.logits[0]\n",
        "        probabilities = torch.softmax(logits, dim=0)\n",
        "        predicted_class = torch.argmax(logits).item()\n",
        "        predicted_label = self.label_map[predicted_class]\n",
        "        return predicted_label, probabilities\n",
        "\n",
        "# Usage\n",
        "classifier = ArticleClassifier()\n",
        "#article = \"Your article goes here...\"\n",
        "predicted_label, probabilities = classifier.classify(article)\n",
        "print(\"Predicted Label:\", predicted_label)\n",
        "print(\"Growth Probability:\", probabilities[1].item())\n",
        "print(\"Not Growth Probability:\", probabilities[0].item())\n"
      ],
      "metadata": {
        "id": "-g3X1y1LwrXa",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "89585fab-c29d-4e34-8c81-a84282377e9c"
      },
      "execution_count": 44,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of the model checkpoint at google/electra-base-discriminator were not used when initializing ElectraForSequenceClassification: ['discriminator_predictions.dense.bias', 'discriminator_predictions.dense.weight', 'discriminator_predictions.dense_prediction.weight', 'discriminator_predictions.dense_prediction.bias']\n",
            "- This IS expected if you are initializing ElectraForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing ElectraForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Some weights of ElectraForSequenceClassification were not initialized from the model checkpoint at google/electra-base-discriminator and are newly initialized: ['classifier.out_proj.bias', 'classifier.out_proj.weight', 'classifier.dense.bias', 'classifier.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Predicted Label: Growth\n",
            "Growth Probability: 0.5368702411651611\n",
            "Not Growth Probability: 0.46312978863716125\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from transformers import ElectraTokenizer, ElectraForSequenceClassification\n",
        "\n",
        "class ArticleClassifier:\n",
        "    def __init__(self):\n",
        "        self.model_name = 'google/electra-base-discriminator'\n",
        "        self.tokenizer = ElectraTokenizer.from_pretrained(self.model_name)\n",
        "        self.model = ElectraForSequenceClassification.from_pretrained(self.model_name)\n",
        "        self.label_map = {0: 'Not Growth', 1: 'Growth'}\n",
        "        self.actionable_label_map = {0: 'Not Actionable', 1: 'Actionable'}\n",
        "\n",
        "    def classify(self, article):\n",
        "        inputs = self.tokenizer.encode_plus(\n",
        "            article,\n",
        "            add_special_tokens=True,\n",
        "            padding='longest',\n",
        "            truncation=True,\n",
        "            return_tensors='pt'\n",
        "        )\n",
        "        outputs = self.model(**inputs)\n",
        "        logits = outputs.logits[0]\n",
        "        probabilities = torch.softmax(logits, dim=0)\n",
        "        predicted_class = torch.argmax(logits).item()\n",
        "        predicted_label = self.label_map[predicted_class]\n",
        "        return predicted_label, probabilities\n",
        "\n",
        "    def classify_actionable(self, article):\n",
        "        predicted_label, probabilities = self.classify(article)\n",
        "        predicted_class = torch.argmax(probabilities).item()\n",
        "        actionable_label = self.actionable_label_map[predicted_class]\n",
        "        return actionable_label, probabilities\n",
        "\n",
        "# Usage\n",
        "classifier = ArticleClassifier()\n",
        "#article = \"Your article goes here...\"\n",
        "predicted_label, probabilities = classifier.classify(article)\n",
        "actionable_label, probabilities = classifier.classify_actionable(article)\n",
        "print(\"Growth Classification:\")\n",
        "print(\"Predicted Label:\", predicted_label)\n",
        "print(\"Growth Probability:\", probabilities[1].item())\n",
        "print(\"Not Growth Probability:\", probabilities[0].item())\n",
        "\n",
        "print(\"\\nActionable Classification:\")\n",
        "print(\"Predicted Label:\", actionable_label)\n",
        "print(\"Actionable Probability:\", probabilities[1].item())\n",
        "print(\"Not Actionable Probability:\", probabilities[0].item())\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fi6t93axrKni",
        "outputId": "6f2f4ea3-8d19-4f0e-a5d3-5c9bfc551f3e"
      },
      "execution_count": 45,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of the model checkpoint at google/electra-base-discriminator were not used when initializing ElectraForSequenceClassification: ['discriminator_predictions.dense.bias', 'discriminator_predictions.dense.weight', 'discriminator_predictions.dense_prediction.weight', 'discriminator_predictions.dense_prediction.bias']\n",
            "- This IS expected if you are initializing ElectraForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing ElectraForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Some weights of ElectraForSequenceClassification were not initialized from the model checkpoint at google/electra-base-discriminator and are newly initialized: ['classifier.out_proj.bias', 'classifier.out_proj.weight', 'classifier.dense.bias', 'classifier.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Growth Classification:\n",
            "Predicted Label: Not Growth\n",
            "Growth Probability: 0.49958986043930054\n",
            "Not Growth Probability: 0.5004101991653442\n",
            "\n",
            "Actionable Classification:\n",
            "Predicted Label: Not Actionable\n",
            "Actionable Probability: 0.49958986043930054\n",
            "Not Actionable Probability: 0.5004101991653442\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Use Labels & score to print the output in Seaborn"
      ],
      "metadata": {
        "id": "bxhtwTXcsLu1"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}